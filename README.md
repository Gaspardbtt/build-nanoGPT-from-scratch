# nanoGPT: Building a Language Model from Scratch (OnGoing) 

This repository contains my implementation of **nanoGPT**, inspired by Andrej Karpathy's YouTube tutorial: [Build nanoGPT](https://www.youtube.com/watch?v=l8pRSuU81PU&t=2150s). The objective is to construct the **GPT-2 (124M)** model from the ground up, starting with an empty file and progressively adding the necessary components.

## Project Overview

In this project, we begin with an empty file and incrementally implement the components required to build the **GPT-2 (124M)** model. The aim is to create a simple language model trained on internet text, capable of generating coherent text after training. This process is highly educational and demonstrates how to develop a deep learning model from scratch. The model can be trained on a cloud GPU in approximately **1 hour** for a cost of around **$10**. Although this project focuses on GPT-2, the principles can be extended to larger models like GPT-3, given sufficient resources.

## Requirements

- **GPU**: Cloud GPU (recommended: **Lambda**)
- **Time**: Approximately 1 hour for GPT-2 (124M)

## Key Features

- **Incremental Development**: Start with an empty file and gradually add components.
- **Educational**: Learn the fundamentals of building a language model from scratch.
- **Cost-Effective**: Train the model on a cloud GPU for around $10.
- **Scalable**: Principles can be applied to larger models with more resources.


## Contributions

Contributions are welcome! Feel free to open issues or submit pull requests to improve the project.

## Acknowledgments

Special thanks to Andrej Karpathy for the inspirational tutorial and guidance.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")  #124M\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "\n",
    "for k,v in sd_hf.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and my project will get better with time, but I think there are a lot more things that can help you\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, so if I don't have a problem, I can fix it by creating new words\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to learn some stuff. I'll try to do some basic programming and just learn better ways\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't believe in grammar. This will work for every language model. You can define it very quickly\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, a model of how things should be, and then we look at different things as well.\" I\\'d like to'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length = 30, num_return_sequences = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fonctionnal' from 'torch.nn' (/opt/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fonctionnal \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'fonctionnal' from 'torch.nn' (/opt/anaconda3/lib/python3.11/site-packages/torch/nn/__init__.py)"
     ]
    }
   ],
   "source": [
    "with open ('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text [:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2982    11  2026  1510 10860   286  9260   547 16318  1497    11   447\n",
      "   251  1605 10099  1757  6748   490  2098  1568    13   564   250  8061\n",
      "  3332]\n"
     ]
    }
   ],
   "source": [
    "        # get the shard filename :) \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_root = \"edu_fineweb10B\"\n",
    "shards = os.listdir(data_root)\n",
    "shard = shards[0]\n",
    "import numpy as np\n",
    "\n",
    "# Chargement du fichier\n",
    "data = np.load(\"edu_fineweb10B/edufineweb_train_000001.npy\", allow_pickle=True)\n",
    "\n",
    "# Affichage\n",
    "print(data[0:25])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " heard, 50 million tons of soil were blown away,” American journalist John Gunther reported later. “People sat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Charger le tokenizer GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ton tableau d'IDs\n",
    "ids = [2982, 11, 2026, 1510, 10860, 286, 9260, 547, 16318, 1497, 11, 447,\n",
    "       251, 1605, 10099, 1757, 6748, 490, 2098, 1568, 13, 564, 250, 8061,\n",
    "       3332]\n",
    "\n",
    "# Décoder en texte\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edu_fineweb10B/edufineweb_train_000001.npy', 'edu_fineweb10B/edufineweb_train_000002.npy', 'edu_fineweb10B/edufineweb_train_000003.npy', 'edu_fineweb10B/edufineweb_train_000004.npy', 'edu_fineweb10B/edufineweb_train_000005.npy', 'edu_fineweb10B/edufineweb_train_000006.npy', 'edu_fineweb10B/edufineweb_train_000007.npy', 'edu_fineweb10B/edufineweb_train_000008.npy', 'edu_fineweb10B/edufineweb_train_000009.npy', 'edu_fineweb10B/edufineweb_train_000010.npy', 'edu_fineweb10B/edufineweb_train_000011.npy', 'edu_fineweb10B/edufineweb_train_000012.npy', 'edu_fineweb10B/edufineweb_train_000013.npy', 'edu_fineweb10B/edufineweb_train_000014.npy', 'edu_fineweb10B/edufineweb_train_000015.npy', 'edu_fineweb10B/edufineweb_train_000016.npy', 'edu_fineweb10B/edufineweb_train_000017.npy', 'edu_fineweb10B/edufineweb_train_000018.npy', 'edu_fineweb10B/edufineweb_train_000019.npy', 'edu_fineweb10B/edufineweb_train_000020.npy', 'edu_fineweb10B/edufineweb_train_000021.npy', 'edu_fineweb10B/edufineweb_train_000022.npy', 'edu_fineweb10B/edufineweb_train_000023.npy', 'edu_fineweb10B/edufineweb_train_000024.npy', 'edu_fineweb10B/edufineweb_train_000025.npy', 'edu_fineweb10B/edufineweb_train_000026.npy', 'edu_fineweb10B/edufineweb_train_000027.npy', 'edu_fineweb10B/edufineweb_train_000028.npy', 'edu_fineweb10B/edufineweb_train_000029.npy', 'edu_fineweb10B/edufineweb_train_000030.npy', 'edu_fineweb10B/edufineweb_train_000031.npy', 'edu_fineweb10B/edufineweb_train_000032.npy', 'edu_fineweb10B/edufineweb_train_000033.npy', 'edu_fineweb10B/edufineweb_train_000034.npy', 'edu_fineweb10B/edufineweb_train_000035.npy', 'edu_fineweb10B/edufineweb_train_000036.npy', 'edu_fineweb10B/edufineweb_train_000037.npy', 'edu_fineweb10B/edufineweb_train_000038.npy', 'edu_fineweb10B/edufineweb_train_000039.npy', 'edu_fineweb10B/edufineweb_train_000040.npy', 'edu_fineweb10B/edufineweb_train_000041.npy', 'edu_fineweb10B/edufineweb_train_000042.npy', 'edu_fineweb10B/edufineweb_train_000043.npy', 'edu_fineweb10B/edufineweb_train_000044.npy', 'edu_fineweb10B/edufineweb_train_000045.npy', 'edu_fineweb10B/edufineweb_train_000046.npy', 'edu_fineweb10B/edufineweb_train_000047.npy', 'edu_fineweb10B/edufineweb_train_000048.npy', 'edu_fineweb10B/edufineweb_train_000049.npy', 'edu_fineweb10B/edufineweb_train_000050.npy', 'edu_fineweb10B/edufineweb_train_000051.npy', 'edu_fineweb10B/edufineweb_train_000052.npy', 'edu_fineweb10B/edufineweb_train_000053.npy', 'edu_fineweb10B/edufineweb_train_000054.npy', 'edu_fineweb10B/edufineweb_train_000055.npy', 'edu_fineweb10B/edufineweb_train_000056.npy', 'edu_fineweb10B/edufineweb_train_000057.npy', 'edu_fineweb10B/edufineweb_train_000058.npy', 'edu_fineweb10B/edufineweb_train_000059.npy', 'edu_fineweb10B/edufineweb_train_000060.npy', 'edu_fineweb10B/edufineweb_train_000061.npy', 'edu_fineweb10B/edufineweb_train_000062.npy', 'edu_fineweb10B/edufineweb_train_000063.npy', 'edu_fineweb10B/edufineweb_train_000064.npy', 'edu_fineweb10B/edufineweb_train_000065.npy', 'edu_fineweb10B/edufineweb_train_000066.npy', 'edu_fineweb10B/edufineweb_train_000067.npy', 'edu_fineweb10B/edufineweb_train_000068.npy', 'edu_fineweb10B/edufineweb_train_000069.npy', 'edu_fineweb10B/edufineweb_train_000070.npy', 'edu_fineweb10B/edufineweb_train_000071.npy', 'edu_fineweb10B/edufineweb_train_000072.npy', 'edu_fineweb10B/edufineweb_train_000073.npy', 'edu_fineweb10B/edufineweb_train_000074.npy', 'edu_fineweb10B/edufineweb_train_000075.npy', 'edu_fineweb10B/edufineweb_train_000076.npy', 'edu_fineweb10B/edufineweb_train_000077.npy', 'edu_fineweb10B/edufineweb_train_000078.npy', 'edu_fineweb10B/edufineweb_train_000079.npy', 'edu_fineweb10B/edufineweb_train_000080.npy', 'edu_fineweb10B/edufineweb_train_000081.npy', 'edu_fineweb10B/edufineweb_train_000082.npy', 'edu_fineweb10B/edufineweb_train_000083.npy', 'edu_fineweb10B/edufineweb_train_000084.npy', 'edu_fineweb10B/edufineweb_train_000085.npy', 'edu_fineweb10B/edufineweb_train_000086.npy', 'edu_fineweb10B/edufineweb_train_000087.npy', 'edu_fineweb10B/edufineweb_train_000088.npy', 'edu_fineweb10B/edufineweb_train_000089.npy', 'edu_fineweb10B/edufineweb_train_000090.npy', 'edu_fineweb10B/edufineweb_train_000091.npy', 'edu_fineweb10B/edufineweb_train_000092.npy', 'edu_fineweb10B/edufineweb_train_000093.npy', 'edu_fineweb10B/edufineweb_train_000094.npy', 'edu_fineweb10B/edufineweb_train_000095.npy', 'edu_fineweb10B/edufineweb_train_000096.npy', 'edu_fineweb10B/edufineweb_train_000097.npy', 'edu_fineweb10B/edufineweb_train_000098.npy', 'edu_fineweb10B/edufineweb_train_000099.npy']\n"
     ]
    }
   ],
   "source": [
    "import os        \n",
    "split = \"train\" # or \"valid\" or \"test\"\n",
    "# get the shard filename :) \n",
    "data_root = \"edu_fineweb10B\"\n",
    "shards = os.listdir(data_root)\n",
    "shards = [s for s in shards if split in s]\n",
    "shards = sorted(shards)\n",
    "shards = [os.path.join(data_root, s) for s in shards]\n",
    "\n",
    "print(shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement hellaswag (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for hellaswag\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hellaswag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install hellaswag\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhellaswag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m render_example\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hellaswag'"
     ]
    }
   ],
   "source": [
    "!pip install hellaswag\n",
    "\n",
    "from hellaswag import render_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
